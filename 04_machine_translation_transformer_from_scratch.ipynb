{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6fa8a39",
   "metadata": {},
   "source": [
    "# Machine Translation with Transformer from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d2f9266",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "import torch.nn as nn\n",
    "from torchtext.datasets import Multi30k\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torchtext.data import functional\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import random\n",
    "import spacy \n",
    "import math\n",
    "import copy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b05864",
   "metadata": {},
   "source": [
    "### Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02a12fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "de_tokenizer = get_tokenizer('spacy', 'de_core_news_lg')\n",
    "en_tokenizer = get_tokenizer('spacy', 'en_core_web_lg')\n",
    "\n",
    "def de_yield_tokens(train_iter):\n",
    "    for de_text, _ in train_iter:\n",
    "        yield de_tokenizer(de_text[:-1].lower())\n",
    "        \n",
    "def en_yield_tokens(train_iter):\n",
    "    for _, en_text in train_iter:\n",
    "        yield en_tokenizer(en_text[:-1].lower())\n",
    "        \n",
    "special_tokens = [\"<pad>\", \"<unk>\", \"<sos>\", \"<eos>\"]\n",
    "\n",
    "train_iter = Multi30k(split=(\"train\"))\n",
    "source_vocab = build_vocab_from_iterator(de_yield_tokens(train_iter), \n",
    "                                         min_freq = 1,\n",
    "                                         specials = special_tokens)\n",
    "source_vocab.set_default_index(source_vocab[\"<unk>\"])\n",
    "\n",
    "train_iter = Multi30k(split=(\"train\"))\n",
    "target_vocab = build_vocab_from_iterator(en_yield_tokens(train_iter),\n",
    "                                        min_freq = 1,\n",
    "                                        specials = special_tokens)\n",
    "target_vocab.set_default_index(target_vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88650705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train_iter, valid_iter, test_iter = Multi30k()\n",
    "\n",
    "en_text_pipeline = lambda x:  target_vocab([\"<sos>\"] + en_tokenizer(x)[:-1] + [\"<eos>\"]) \n",
    "de_text_pipeline = lambda x: source_vocab([\"<sos>\"] + de_tokenizer(x)[:-1] + [\"<eos>\"])\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "def collate_batch_input(batch):\n",
    "    source_list, target_list = [], []\n",
    "    for source_text, target_text in batch:  \n",
    "        text_seq = de_text_pipeline(source_text.lower()) \n",
    "        source_list.append(torch.tensor(text_seq, dtype=torch.int64))\n",
    "        text_seq = en_text_pipeline(target_text.lower())\n",
    "        target_list.append(torch.tensor(text_seq[:-1], dtype=torch.int64))\n",
    "               \n",
    "    source_tensor = pad_sequence(source_list, batch_first=True, padding_value=source_vocab[\"<pad>\"])\n",
    "    target_tensor = pad_sequence(target_list, batch_first=True, padding_value=target_vocab[\"<pad>\"])\n",
    "    \n",
    "    return source_tensor, target_tensor, \n",
    "\n",
    "\n",
    "train_dataset = functional.to_map_style_dataset(train_iter)\n",
    "valid_dataset = functional.to_map_style_dataset(valid_iter)\n",
    "test_dataset = functional.to_map_style_dataset(test_iter)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size = BATCH_SIZE,\n",
    "                          shuffle = True, collate_fn = collate_batch_input)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size = BATCH_SIZE,\n",
    "                          shuffle=False, collate_fn = collate_batch_input)\n",
    "test_loader = DataLoader(test_dataset, batch_size = BATCH_SIZE,\n",
    "                          shuffle=False, collate_fn = collate_batch_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2fdebf",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1997bc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A standard Encoder-Decoder architecture. Base for this and many \n",
    "    other models.\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder, decoder, src_embed, trg_embed, generator):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.trg_embed = trg_embed\n",
    "        self.generator = generator\n",
    "        \n",
    "    def forward(self, src, trg, src_mask, trg_mask):\n",
    "        \"Take in and process masked src and target sequences.\"\n",
    "        dec_out = self.decode(trg, self.encode(src, src_mask), trg_mask, src_mask)\n",
    "        return self.generator(dec_out)\n",
    "    \n",
    "    def encode(self, src, src_mask):\n",
    "        return self.encoder(self.src_embed(src), src_mask)\n",
    "    \n",
    "    def decode(self, trg, enc_src, trg_mask, src_mask):\n",
    "        return self.decoder(self.trg_embed(trg), enc_src, trg_mask, src_mask)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8dccfb4",
   "metadata": {},
   "source": [
    "<img src=\"img/transformer1.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d9e548",
   "metadata": {},
   "source": [
    "### Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "dd7f5e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    \"\"\" Combining word embedding and positional embedding\"\"\"\n",
    "    def __init__(self, vocab, d_model, pad_index):\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab, d_model, padding_idx = pad_index)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        e = self.embedding(x)\n",
    "        pos_emb = torch.zeros(e.size(0), e.size(1), e.size(2))\n",
    "        position = torch.arange(0, x.size(1)).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, self.d_model, 2) *\n",
    "                             -(math.log(10000.0) / self.d_model))\n",
    "        pos_emb[:, :, 0::2] = torch.sin(position * div_term)\n",
    "        pos_emb[:, :, 1::2] = torch.cos(position * div_term)\n",
    "        return  e + pos_emb\n",
    "\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1799b104",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \"Define standard linear + softmax generation step.\"\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Generator, self).__init__()\n",
    "        self.proj = nn.Linear(d_model, vocab)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.log_softmax(self.proj(x), dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3bd57a",
   "metadata": {},
   "source": [
    "## Encoder and Decoder Stacks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c359f3",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "28d49a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clones(module, N):\n",
    "    \"Produce N identical layers.\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a5328632",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Stack of N EncoderLayers\n",
    "    \"\"\"\n",
    "    def __init__(self, layer, N):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        \n",
    "    def forward(self, src, src_mask):\n",
    "        \"Pass the input (and mask) through each layer in turn.\"\n",
    "        for layer in self.layers:\n",
    "            src = layer(src, src_mask)\n",
    "        return src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "47f7836d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder is made up of self-attn and feed forward.\n",
    "    \"\"\"\n",
    "    def __init__(self, size, self_attn, feed_forward):\n",
    "        super(EncoderBlock, self).__init__()\n",
    "        self.layer_norm_1 = nn.LayerNorm(size)\n",
    "        self.layer_norm_2 = nn.LayerNorm(size)\n",
    "        self.self_attn = self_attn\n",
    "        self.feedforward = feed_forward\n",
    "        \n",
    "    def forward(self, src, src_mask):\n",
    "        out = self.layer_norm_1(src + self.self_attn(src, src, src, src_mask))\n",
    "        return self.layer_norm_2(out + self.feedforward(out))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba7ed6f",
   "metadata": {},
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "3f07d727",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, h, d_model):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.Q_linear = nn.Linear(d_model, d_model)\n",
    "        self.K_linear = nn.Linear(d_model, d_model)\n",
    "        self.V_linear = nn.Linear(d_model, d_model)\n",
    "        self.lin_out = nn.Linear(d_model, d_model)\n",
    "        self.h = h\n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_model // h\n",
    "        \n",
    "    def forward(self, query, key, value, mask=None):  \n",
    "        Q = self.Q_linear(query)\n",
    "        K = self.Q_linear(key)\n",
    "        V = self.Q_linear(value)\n",
    "        \n",
    "        n_batches = Q.size(0)\n",
    "    \n",
    "        # Splitting in attention heads\n",
    "        Q = Q.view(n_batches, -1, self.h, self.d_k)\n",
    "        K = K.view(n_batches, -1, self.h, self.d_k)\n",
    "        V = V.view(n_batches, -1, self.h, self.d_k)\n",
    "        Q = Q.transpose(1,2)\n",
    "        K = K.transpose(1,2)\n",
    "        V = V.transpose(1,2)\n",
    "        x = self.compute_attention(Q, K, V, mask=mask)\n",
    "        # concatenating attention heads\n",
    "        x = x.transpose(1,2).contiguous().view(n_batches, -1, self.h*self.d_k)\n",
    "        return self.lin_out(x)\n",
    "        \n",
    "    @staticmethod\n",
    "    def compute_attention(query, key, value, mask):\n",
    "        d_k = query.size(-1)\n",
    "        scores = torch.matmul(query, key.transpose(-2,-1)) / math.sqrt(d_k)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask==0, -np.inf)\n",
    "        p_attn = F.softmax(scores, dim=-1)\n",
    "        return torch.matmul(p_attn, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590d1fa3",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/transformers-explained-visually-part-3-multi-head-attention-deep-dive-1c1ff1024853"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "694229ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.w_2(F.relu(self.w_1(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038abd8b",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "246595b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Stack of N DecoderBlocks\n",
    "    \"\"\"\n",
    "    def __init__(self, layer, N):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        \n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "        \"Pass the input (and mask) through each layer in turn.\"\n",
    "        for layer in self.layers:\n",
    "            trg = layer(trg, enc_src, trg_mask, src_mask)\n",
    "        return trg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "870d1b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder is made up of self-attn, src_attn and feed forward.\n",
    "    \"\"\"\n",
    "    def __init__(self, size, self_attn, src_attn, feed_forward):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        self.layer_norm_1 = nn.LayerNorm(size)\n",
    "        self.layer_norm_2 = nn.LayerNorm(size)\n",
    "        self.layer_norm_3 = nn.LayerNorm(size)\n",
    "        self.self_attn = self_attn\n",
    "        self.src_attn = src_attn\n",
    "        self.feedforward = feed_forward\n",
    "        \n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "        out = self.layer_norm_1(trg + self.self_attn(trg, trg, trg, trg_mask))\n",
    "        out = self.layer_norm_2(out + self.src_attn(trg, enc_src, enc_src, src_mask))\n",
    "        return self.layer_norm_3(out + self.feedforward(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "873822ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(d_model, h, src_vocab_size, trg_vocab_size, src_pad_idx, \n",
    "               trg_pad_idx, n_layers):\n",
    "    src_embedding = Embeddings(src_vocab_size, d_model, src_pad_index)\n",
    "    trg_embedding = Embeddings(trg_vocab_size, d_model, trg_pad_idx)\n",
    "    generator = Generator(d_model, len(target_vocab))\n",
    "    attention = MultiHeadAttention(h, d_model)\n",
    "    ff = PositionwiseFeedForward(64, 128)\n",
    "    enc_block = EncoderBlock(d_model, attention, ff)\n",
    "    encoder = Encoder(enc_block, n_layers)\n",
    "    dec_block = DecoderBlock(d_model, attention, attention, ff)\n",
    "    decoder = Decoder(dec_block, n_layers)\n",
    "    transformer = EncoderDecoder(encoder, decoder, src_embedding, trg_embedding, generator)\n",
    "    return transformer\n",
    "\n",
    "def make_src_mask(src_pad_index, src):        \n",
    "        #src = [batch size, src len]\n",
    "        \n",
    "        src_mask = (src != src_pad_index).unsqueeze(1).unsqueeze(2)\n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "\n",
    "        return src_mask\n",
    "    \n",
    "def make_trg_mask(trg_pad_index, trg):        \n",
    "        #trg = [batch size, trg len]\n",
    "        \n",
    "        trg_pad_mask = (trg != trg_pad_index).unsqueeze(1).unsqueeze(2)       \n",
    "        #trg_pad_mask = [batch size, 1, 1, trg len]\n",
    "        \n",
    "        trg_len = trg.shape[1]      \n",
    "        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len))).bool()     \n",
    "        #trg_sub_mask = [trg len, trg len]\n",
    "            \n",
    "        trg_mask = trg_pad_mask & trg_sub_mask    \n",
    "        #trg_mask = [batch size, 1, trg len, trg len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c1cd0862",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = iter(train_loader)\n",
    "a = next(a)\n",
    "d_model = 64\n",
    "h = 4\n",
    "src_vocab_size = len(source_vocab)\n",
    "trg_vocab_size = len(target_vocab)\n",
    "src_pad_index = source_vocab[\"<pad>\"]\n",
    "trg_pad_index = target_vocab[\"<pad>\"]\n",
    "n_layers = 3\n",
    "\n",
    "transformer = make_model(d_model, h, src_vocab_size, trg_vocab_size, \n",
    "                         src_pad_index, trg_pad_index, n_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "9aabadfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 2,659,587 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(transformer):\n",
    "    return sum(p.numel() for p in transformer.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(transformer):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "8d83ad41",
   "metadata": {},
   "outputs": [],
   "source": [
    "src = a[0]\n",
    "trg = a[1]\n",
    "src_mask = make_src_mask(src_pad_index, src)\n",
    "trg_mask = make_trg_mask(trg_pad_index, trg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91cabf9",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a4e79843",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.0005\n",
    "\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr = LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "4afa3912",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index = trg_pad_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "5ff19b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i, (src, trg) in enumerate(iterator):\n",
    "        \n",
    "        src_mask = make_src_mask(src_pad_index, src)\n",
    "        trg_mask = make_trg_mask(trg_pad_index, trg[:,:-1])\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(src, trg[:,:-1], src_mask, trg_mask)\n",
    "                \n",
    "        #output = [batch size, trg len - 1, output dim]\n",
    "        #trg = [batch size, trg len]\n",
    "            \n",
    "        output_dim = output.shape[-1]\n",
    "            \n",
    "        output = output.contiguous().view(-1, output_dim)\n",
    "        trg = trg[:,1:].contiguous().view(-1)\n",
    "                \n",
    "        #output = [batch size * trg len - 1, output dim]\n",
    "        #trg = [batch size * trg len - 1]\n",
    "            \n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "ec31b2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i, (src, trg) in enumerate(iterator):\n",
    "\n",
    "            src_mask = make_src_mask(src_pad_index, src)\n",
    "            trg_mask = make_trg_mask(trg_pad_index, trg[:,:-1])\n",
    "\n",
    "            output = model(src, trg[:,:-1], src_mask, trg_mask)\n",
    "            \n",
    "            #output = [batch size, trg len - 1, output dim]\n",
    "            #trg = [batch size, trg len]\n",
    "            \n",
    "            output_dim = output.shape[-1]\n",
    "            \n",
    "            output = output.contiguous().view(-1, output_dim)\n",
    "            trg = trg[:,1:].contiguous().view(-1)\n",
    "            \n",
    "            #output = [batch size * trg len - 1, output dim]\n",
    "            #trg = [batch size * trg len - 1]\n",
    "            \n",
    "            loss = criterion(output, trg)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "9cf974f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "9651cd3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 8m 6s\n",
      "\tTrain Loss: 3.017 | Train PPL:  20.434\n",
      "\t Val. Loss: 2.875 |  Val. PPL:  17.728\n",
      "Epoch: 02 | Time: 8m 1s\n",
      "\tTrain Loss: 2.654 | Train PPL:  14.214\n",
      "\t Val. Loss: 2.571 |  Val. PPL:  13.085\n",
      "Epoch: 03 | Time: 8m 53s\n",
      "\tTrain Loss: 2.327 | Train PPL:  10.249\n",
      "\t Val. Loss: 2.289 |  Val. PPL:   9.868\n",
      "Epoch: 04 | Time: 8m 4s\n",
      "\tTrain Loss: 2.016 | Train PPL:   7.510\n",
      "\t Val. Loss: 2.014 |  Val. PPL:   7.493\n",
      "Epoch: 05 | Time: 8m 33s\n",
      "\tTrain Loss: 1.722 | Train PPL:   5.598\n",
      "\t Val. Loss: 1.761 |  Val. PPL:   5.819\n",
      "Epoch: 06 | Time: 8m 44s\n",
      "\tTrain Loss: 1.461 | Train PPL:   4.312\n",
      "\t Val. Loss: 1.552 |  Val. PPL:   4.722\n",
      "Epoch: 07 | Time: 8m 34s\n",
      "\tTrain Loss: 1.236 | Train PPL:   3.441\n",
      "\t Val. Loss: 1.376 |  Val. PPL:   3.958\n",
      "Epoch: 08 | Time: 9m 12s\n",
      "\tTrain Loss: 1.043 | Train PPL:   2.838\n",
      "\t Val. Loss: 1.210 |  Val. PPL:   3.352\n",
      "Epoch: 09 | Time: 8m 50s\n",
      "\tTrain Loss: 0.880 | Train PPL:   2.411\n",
      "\t Val. Loss: 1.096 |  Val. PPL:   2.991\n",
      "Epoch: 10 | Time: 9m 9s\n",
      "\tTrain Loss: 0.744 | Train PPL:   2.104\n",
      "\t Val. Loss: 0.995 |  Val. PPL:   2.705\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 10\n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train(transformer, train_loader, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(transformer, valid_loader, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(transformer.state_dict(), 'tut6-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a702d469",
   "metadata": {},
   "source": [
    "#### Reference\n",
    "1. https://github.com/bentrevett\n",
    "2. https://jalammar.github.io/illustrated-transformer/\n",
    "3. https://medium.com/analytics-vidhya/masking-in-transformers-self-attention-mechanism-bad3c9ec235c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c34661c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
